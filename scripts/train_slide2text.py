#!/usr/bin/env python
# coding: utf-8
"""
scripts/train_slide2text.py

ÊúÄÂ∞è DemoÔºöÁî® H5 ÁâπÂæÅÔºàMean PoolÔºâ+ Á∫øÊÄßÊò†Â∞Ñ ÂâçÁºÄÂåñ ÈÄÅÂÖ• GPT2 ÁîüÊàêÊä•Âëä
ÊîØÊåÅ‰∏§ÁßçÊä•ÂëäÊñá‰ª∂Ê†ºÂºèÔºö
 1. JSON Êï∞ÁªÑ: [ {‚Ä¶}, {‚Ä¶}, ‚Ä¶ ]
 2. JSONL:    ÊØèË°å‰∏Ä‰∏™ {‚Ä¶}
"""
import os
# ‚Äî‚Äî‚Äî‚Äî Âú®ÂØºÂÖ• HF/transformers ‰πãÂâçËÆæÁΩÆ ‚Äî‚Äî‚Äî‚Äî
# Â¶ÇÊûú‰Ω†Âú®Áî®ÂÜÖÈÉ®ÈïúÂÉèÔºå‰∏çË¶ÅÊää token ‰º†ÁªôÂÆÉÔºö
os.environ["HF_HUB_DISABLE_SSL_VERIFY"]  = "1"
os.environ["HF_ENDPOINT"]                = "https://hf-mirror.com"
import re
import json
import h5py
import torch
from pathlib import Path
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments
)
from datasets import Dataset

# ‚Äî‚Äî Áî®Êà∑ÈÖçÁΩÆÂå∫ ‚Äî‚Äî #
SLIDE_DIR  = Path("./data/ultralowres_h5")      # H5 ÁâπÂæÅÊñá‰ª∂Â§π
REPORTS    = Path("./data/HCC_translated.json") # JSON Êï∞ÁªÑÊàñ JSONL
FEAT_DIM   = 192                                # ÁâπÂæÅÁª¥Â∫¶
PREFIX_DIM = 768                                # GPT2 hidden size
MAX_LEN    = 512                                # ÊñáÊú¨ÊúÄÂ§ßÈïøÂ∫¶
BATCH_SIZE = 4                                  # Trainer batch size
EPOCHS     = 1                                  # ËÆ≠ÁªÉËΩÆÊï∞
LR         = 2e-5                               # Â≠¶‰π†Áéá
OUTPUT_DIR = "outputs/slide2text_demo"          # ËæìÂá∫ÁõÆÂΩï
MODEL_NAME = "gpt2"                             # English GPT-2
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî #

def load_reports(path):
    """
    ËØªÂèñ REPORTS Êñá‰ª∂ÔºåÊîØÊåÅÔºö
      - Êï¥‰∏™ÊòØ‰∏Ä‰∏™ JSON Êï∞ÁªÑ
      - ÊàñËÄÖÊØèË°å‰∏Ä‰∏™ JSON ÂØπË±°ÔºàJSONLÔºâ
    ËøîÂõû { base_id: text }
    """
    text = path.read_text(encoding="utf-8").strip()
    # ÂÖàÂ∞ùËØïÂΩì JSON Êï∞ÁªÑ
    try:
        arr = json.loads(text)
        if not isinstance(arr, list):
            raise ValueError
    except Exception:
        # ÈÄÄÂõûÂà∞ JSONLÔºöÊåâË°å parse
        arr = []
        for idx, line in enumerate(text.splitlines(), 1):
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError:
                print(f"‚ö†Ô∏è Skip invalid JSONL at line {idx}")
                continue
            arr.append(obj)

    # Êää diagnosis + findings ÊãºÊàê‰∏ÄÂè•ÊñáÊú¨ÔºåÂøΩÁï• immuno
    pattern = re.compile(r"^S?(\d{4}-\d{6})")
    id2text = {}
    for obj in arr:
        sid = obj.get("slide_id", "")
        m = pattern.match(sid)
        if not m:
            continue
        base = m.group(1)
        diag = obj.get("diagnosis", "").strip()
        fin  = obj.get("findings", "").strip()
        if diag and fin:
            id2text[base] = f"{diag}„ÄÇ{fin}"
    return id2text

def build_records(slide_dir, id2text):
    """
    ÈÅçÂéÜ slide_dir/*.h5ÔºåÊåâÊ≠£ÂàôÊèêÂèñ base_idÔºå
    Â¶ÇÊûúÂú® id2text ÈáåÔºåÂ∞±Âä†ÂÖ• records ÂàóË°®„ÄÇ
    """
    recs = []
    for h5_path in slide_dir.glob("*.h5"):
        m = re.match(r"^S?(\d{4}-\d{6})", h5_path.stem)
        if not m:
            continue
        base = m.group(1)
        txt  = id2text.get(base)
        if txt:
            recs.append({"feat_path": str(h5_path), "text": txt})
    return recs

def load_features(ex):
    """
    Dataset.map Áî®ÂáΩÊï∞ÔºöÊâìÂºÄ H5ÔºåMean Pool features
    """
    with h5py.File(ex["feat_path"], "r") as h5:
        feats = h5["features"][:]      # (N, FEAT_DIM)
        mf    = feats.mean(axis=0)     # (FEAT_DIM,)
    return {"feat": mf.astype("float32"), "text": ex["text"]}

def main():
    # 1) ËØªÊä•Âëä„ÄÅÂåπÈÖç slide_id
    id2text = load_reports(REPORTS)
    recs    = build_records(SLIDE_DIR, id2text)
    print(f"üîó matched {len(recs)} slide-report pairs")
    if not recs:
        print("‚ùå Ê≤°ÊúâÂåπÈÖçÂà∞‰ªª‰Ωï pairÔºåËØ∑Ê£ÄÊü• REPORTS Ë∑ØÂæÑ‰∏é slide_id Ê†ºÂºè")
        return

    # 2) ÊûÑÈÄ† HF Dataset
    ds = Dataset.from_list(recs)
    # ‚Üê‚Äî‚Äî ËøôÈáåÂè™ remove feat_pathÔºå‰øùÁïô text Âàó ‚Äî‚Äî removed `"text"` from remove_columns
    ds = ds.map(load_features,
                remove_columns=["feat_path"],
                new_fingerprint="feat_loaded")

    # 3) Tokenizer & LM
    tok = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        model_max_length=MAX_LEN,
        use_fast=True
    )
    tok.pad_token = tok.eos_token

    lm = AutoModelForCausalLM.from_pretrained(MODEL_NAME).half().cuda()

    # 4) ÁâπÂæÅÊò†Â∞ÑÂ±Ç FEAT_DIM->PREFIX_DIM
    mapper = torch.nn.Linear(FEAT_DIM, PREFIX_DIM, bias=False).half().cuda()

    # 5) Â∞ÅË£Ö Module
    class Slide2Text(torch.nn.Module):
        def __init__(self, lm, mapper):
            super().__init__()
            self.lm     = lm
            self.mapper = mapper

        def forward(self, feat, input_ids=None, attention_mask=None, labels=None):
            pref = self.mapper(feat).unsqueeze(1)  # (B,1,D)
            emb  = self.lm.transformer.wte(input_ids)
            emb  = torch.cat([pref, emb], dim=1)
            if attention_mask is not None:
                one = torch.ones((attention_mask.size(0),1),
                                 device=attention_mask.device)
                attention_mask = torch.cat([one, attention_mask], dim=1)
            return self.lm(inputs_embeds=emb,
                           attention_mask=attention_mask,
                           labels=labels)

    model = Slide2Text(lm, mapper)

    # 6) Data collator
    def collate_fn(batch):
        feats = torch.tensor([b["feat"] for b in batch]).half().cuda()
        texts = [b["text"] for b in batch]       # now text still exists!
        enc   = tok(texts,
                    padding=True,
                    truncation=True,
                    max_length=MAX_LEN,
                    return_tensors="pt")
        return {
            "feat":           feats,
            "input_ids":      enc.input_ids.cuda(),
            "attention_mask": enc.attention_mask.cuda(),
            "labels":         enc.input_ids.cuda(),
        }

    # 7) ËÆ≠ÁªÉÂèÇÊï∞
    args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=BATCH_SIZE,
        num_train_epochs=EPOCHS,
        learning_rate=LR,
        fp16=True,
        save_total_limit=2,
        logging_steps=10,
        report_to=["none"],
    )

    # 8) Trainer & ËÆ≠ÁªÉ
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=ds,
        data_collator=collate_fn,
    )
    trainer.train()
    print("‚úÖ Training complete!")

if __name__ == "__main__":
    main()
